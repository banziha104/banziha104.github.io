

# 회귀분석

- 지도학습 : Y = f(X)에 대하여 입력 변수와 출력 변수(Y)에 관계에 대하여 모델링하는것 

- 회귀(regression) : 입력 변수 X에 대해서 연속형 출력 변수 Y를 예측 
    -   Y = f(X)
        - f : 회귀 모형
        - X : 연속/이산형 변수
        - Y : 연속형 변수
- 분류(classification) : 입력 변수 X에대해서 이산형 출력 변수 Y(class)를 예측
   -   Y = f(X)
        - f : 회귀 모형
        - X : 연속/이산형 변수
        - Y : 이산형 변수 (클래스)

- 회귀분석 : 입력 변수인 X인 정보를 활용하여 출력 변수인 Y를 예측하는 방법
    - 회귀분석중 간단한 방법으로는 선형회귀분석이 있음. 복잡한 방법으로는 비선형 회귀가 있음
    - 대부분의 분류모델 (SVM, Decisiton Tree 등)으로도 회귀가 가능함.
    
- 단순 선형 회귀분석 
    - Y = B_0 + B_1*X+e
    - B_0는 절편 B_1은 기울기이며, 이를 합쳐서 회귀계수로도 불림
    - 가정 : X와 Y는 선형관계 , Y가 정규분포를 따름.
    
- 단순 선형 회귀 분석
    - 알고싶은식 = Y = B_0 + B_1X + e
    - 추정식 = ^Y = ^B_0 + ^B_1X 
    - 좋은 선형 : 직선과 데이터의 차이가 평균적으로 가장 작아지는 직선 
        - 잔차(rasiual) : e_i =y_i - ^y_i 
        - 잔차를 최소화하는 방향으로 추정 
        - 잔차의 제곱합을 최소화하는 구성 : ![latex](https://latex.codecogs.com/gif.latex?\sum_{i=1}^{n}&space;e_i^2) 
            - 잔차의 제곱합을 최소화 시키는 이유
                - 잔차의 합이 0이되는 해는 무수히 많음.
                - 잔차의 절대값의 합은 미분이 불가능한 형태
                - 잔차의 제곱합은 미분이 가능한 형태로 유일한 해를 찾을 수 있음.
                
                
<br>

# 회귀계수 추정

- SSE 를 ![latex](https://latex.codecogs.com/gif.latex?\hat{β_0}) 와 ![latex](https://latex.codecogs.com/gif.latex?\hat{β_1})로 편미분하여 연립방정식을 푸는 방법 (Least Square Method))
- ![latex](https://latex.codecogs.com/gif.latex?\therefore&space;\widehat{\beta_0}&space;=&space;\overline{y}-\hat{\beta_1}\overline{x})
- ![latex](https://latex.codecogs.com/gif.latex?\therefore&space;\widehat{\beta_1}&space;=&space;\frac{\sum_{i=1}^n&space;(x_i&space;-&space;\overline{x})(y_i-\overline{y}))}{\sum_{i=1}^n&space;(x_i&space;-&space;\overline{x})^2})

<br>

# 회귀계수의 의미 
                
- 회귀계수의 해석
    - ^B_1의 해석 -X1이 1단위 증가할때마다 y가 ^B_1만큼 증가한다.

- 선형 회귀의 정확도 평가
    - 선형회귀는 잔차의 제곱합(SSE)를 최소화 하는 방법으로 회귀 계수를 추정
    - 즉, SSE가 작으면 작을수록 좋은 모델이라고 볼 수 있음.
    - MSE는 SSE를 표준화한 개념.
    - SST(Total sum of squares) 
    - SSE( Error sum of squares)
    - SSR(Regressiomn sum of squares)  
    - 회귀 분석은 결국 Y의 변동성을 얼마나 독립변수가 잘 설명하느냐가 중요
    - 변수가 여러 개일 때 각각 Y를 설명하는 변동성이 크면 좋은 변수 -> p-value 자연스레 낮아짐.
    
- R^2(R squares)
    - R^2 = SSR / SST 
    - 0~1의 범위를 가짐
    - 입력 변수인 X로 설명할 수 있는 Y를 의미
    - 1에 가까울 수록 정확함
    
    
<br>

# 회귀계수에 대한 검정

- 단순 선형 회귀분석의 검정
- B_1의 표준오차 : S.E(^B_1) = ∂/√S_x_x 
- 귀무가설 : B_1 = 0 (회귀계수는 0이다, 즉 변수의 설명력이 없다)
- 대립가설 : B_1 != 0 (회귀 계수는 0이 아니다, 즉 변수의 설명력이 존재 한다.)
- t분포 (귀무가설 B_1 = 0이다 기준) = ^B_1 / s.e(^B1) 

    

<br>

# 다중 선형 회귀분석 

- 단순선형회귀 분석과 거의 동일, SSE를 최소화 하는 방향으로 추정 
- 역행렬이 구해지지 않을때, 문제가생김. (역행렬은 x끼리 관계가 밀접해질떄 안구해질)
- 다중 선형 회귀 모델 검정   
    - 귀무가설 : B_1 = B_2..B_p =0 (모든 회귀계수는 0이다, 즉 변수의 설명력이 하나도 존재 하지 않는다 )
    - 대립가설 : 하나의 회귀계수라도 0이 아니다. ( 즉 설명력이 있는 변수가 존재한다.)
    - F검정을 통해서 검정 
    - 귀무가설을 기각하기 너무 쉬운 가설, 변수가 추가 되면 될수록 기각하기 쉬워짐.
    - 제곱합의 형태이기 때문에 변수가 추가되면 추가될수록 자연스레 증가.
    - 제곱합의 형태로 검정을 하는 F검정의 특성상 변수가 추가되면 자연스레 기각하기 쉬워진다.

<br>

# 다중 공선성 

> 독립변수들이 강한 선형관계에 엤을때 다중공선성이 있다고 한다.

- 각 변동성이 겹침. 겹치는 변동성에 대해서는 중복으로 가져가지 못함.
- 잘못된 변수해석, 예측 정확도 하락 등을 야기시킨다.
- 다중공선성성을 진단하는 방법
    - VIF(Variance inflation factor), 변수들간의 Correlation 등으로 진단.
        - 1 / 1 - R^2 = R^2 > 0.9 이상인경우 
        - VIF가 10 이상인 경우 다중 공선성이있는 변수라고 판단. (메뉴얼은 아님)
        - VIF_1의 의미 : 다른 변수의 선형결합으로 X1을 설명할 수 있는 정도
    - 상관행렬 및 산점도를 보고 판단 
    - Feature Selection : 중요 변수만 선택하는 방법
    - 단순히 변수를 제거하는 방법 
        - Lasso
        - Stepwise
        - 기타 변수 선택 알고리즘 
    - 변수를 줄이지 않고 활용하는 방법
        - AutoEncoder등의 Feature extraction 기법 (딥러닝 기법)
        - PCA
        - Ridge

- Adjust R^2
    - R^2가 변수가 많아지면 쓸모가 없어짐.
    - R2에 변수 수만큼 penalty를 주는 지표가 Adjusted R2
    - 1 - (SSE/(n-p)) / (SST/(n-1))
    
- AICㅌ
    - 모델의 성능지표로서 MSE에 변수 수 만큼 penalty를 주는 지표
    - 일반적으로 회귀분석에서 Model Selection 할 때 많이 쓰이는 지표
    - AIC = n ln(SSE/n) + 2(p+1)
- BIC
    - AIC 단점은 표본 n이 커질 때 부정확해짐
    - 이를 보완한 지표가 BIC(AIC와 큰 차이는 없음)
 
<br>

# 모형의 성능지표

- Regression
    - MSE (Mean Squared Error) : 실제 종속 변수와 예측한 종속 변수간의 차이 
        - MSE가 작을 수록 좋지만, MSE를 과도하게줄이면 과적합 우려
    - MAPE(mean abosulte )  : 퍼센트 값을 가지며 0에 가까울수록 회귀 모형이 성능이 좋다고 해석할 수 있음.
        - 0~100% 사이를 가져 이해하기 쉬우므로 성능 비교 해석이 가능
- Classification
    - 정확도(Accuracy) : 옳게 분류된 데이터의 수 / 전체 데이터의  수 
    - 정밀도(Precision) : 옳게 분류된 불량 데이터의 수 . 불량으로 예측한 데이터
    - 재현율(Recall) : 옳게 분류된 불량데이터의 수 /실제 불량 데이터의 수
    - 특이도(Specificity) : 옳게 분류된 정상 데이터의 수/ 실제 정상 데이터의 수
    - G-mean : √recall*specificity
    - F1 measure : 2 * (precision * recall)  / (precision + recall)
    - ROC curve : 가로축을 특이도, 세로축을 재현율로 하여 시각화한 그래프
    - ROC curve 의 면적을 AUC
    
<br>

# 변수 선택법

- 변수가 여러 개 일때 최적의 변수 조합을 찾아내는 기법
- 변수의 수가 p개 일 때 변수의 총 조합은 2^p으로 변수 수가 증가함에 따라 변수 조합의 수는 기하급수적으로 증가
- 총 변수들의 조합 중 최적의 조합을 찾기 위한 차선의 방법
- 방법
    1. Feedforward Selection 방법 : 변수를 추가해가며 성능지표를 비교해가는 방법 , AIC가 높아지면 스탑하고 해당 되는 변수 수를 채택, 가장 많이 사용됨.
    2. Backward Elimination 방법 : 변수를 제거해가며 성능지표를 비교해가는 방법 , AIC가 높아지면 스탑하고 해당 되는 변수 수를 채택
    3. Stepwise 
        - 가장 유의한 변수를 추가하거나 유의하지 않는 변수를 제거하는 방법
        - 전진선택법을 사용할 때 한 변수가 선택되면 이미 선택된 변수 중 중요하지 않은 변수가 있을 수 있음
        - 전진석택법의 각 단계에서 이미 선택된 변수들의 중요도를 다시 검사하여 중요하지 않은 변수를 제거하는 방법
        - 일반적으로 가장 널리 쓰이는 방법
            1. 변수 입력/제거를 위한 p-value 임계치 설정
            2. Fward selection을 통한 변수 선정
            3. 선택된 변수 중 유의미한 변수를 남기고 제거, 2-3반복
            4. 변수가 추가되거나 제거할 케이스가 없는 경우 종료
- 교호작용 
    - 변수 간의 시너지 효과 : X1와 X2는 Y에 영향을 끼치지는 않지만, X1과 X2가 결합됨으로써 Y에 중요한 영향을 끼칠 수 있음
    - 교호작용은 일반적으로 도메인지식에 근거하여 추가하여함
- 명목형 변수 : 성별,대학,지역 등 명목형 변수의 경우 전처리가 필요함
    - X(학력)과 Y(수입의 관계) 등 


<br>

# 다중회귀모형에 대한 검증

- 잔차의 가정 
    - 정규성 : 데이터가 모형 가정을 만족하면 분석결과로 나온 잔차는 정규분포를 따라야 한다.
    - 독립성  : 독립변수 x 간에 상관관계가 없는 성질
    - 등분산성 : 잔차가 똑같은 분산을 가진다.
- 회귀모델을 잘 만들었을 경우 잔차는 정규분포를 따른다
- 일반적으로 Residual산점도, Normal Q-Q Plot(Quantiles으로 판단), Residual vs fitted Plot으로 진단 

# 다항회귀분석

> 고차항 이상을 넣은 비선형 모델 

- 필요한 경우
    - 독립변수와 종속변수간의 비선형관계를 가질 경우 - 독립변수와 종속변수의 Plot을 통해 확인 가능
    - 다중 회귀의 가정이 위배된 경우 - Residual Plot을 통해 확인
- 다항회귀 적합
    - 선형분석과 동일
    - X1과 Y의 관계가 2차식 같은 비선형이라면, 2차항만 고려하는것이 일반적 
    - 다항회귀시 고려할 것, 항이 추가될수록 overfitting이 일어날 가능성이 크기 때문에 고차항을 추가시에는 신중해야함.
    - 대부분 2차항만 넣음.

