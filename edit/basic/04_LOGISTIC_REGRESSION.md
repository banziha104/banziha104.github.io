# 로지스틱 회귀분석

> 로지스틱 회귀는 출력 변수를 직접 예측하는 것이 아니라, 두개의 카테고리를 가지는 binary 형태의 출력 변수를 예측할떄 사용

- p = P(success | k 개의 입력 변수 )
- 1 - p = P(fail | k개의 입력 변수.)
- 로지스틱 회귀에서는 k개의 입력변수를 사용하여 성공 실패를 예측하기 위해 성공확률 p(x)를 모델링함
- p(X) = P(success |X_1...,X_k) = B_0 + B_1X_1 +... B_xX_k + randome error(e)
- 로지스틱함수 
    - 왼쪽항에 자연 로그를 취해줌
    - 확률은 max가 1이므로 ln(p(X))가 무한대 값을 가질 수 없음
    - logit : ln(p(X)/1-p(X)) =  B_0 + B_1X_1 +... B_xX_k + randome error(e)
    - p(X) = e^x / (1+e)^x 
    - 베르누이 분포를 이용하여 추정
    - 최소제곱법이 아닌 최대우도법을 사용
    - RF_impedance 값이 높아질 수록 불량일 확률이 낮다는 결과 
    - Logit : RF_impedence가 1단위 증가할떄 logit이 x단위만큼 증가한다
    - Odds : RF_impedance가 1단위 증가할때 불량일 확률이 x배 증가한다.
    

<br>

## 회귀계수 축소법

- 분석용 데이터의 이상적 조건
    - 독립변수 X사이에 상관성이 작아야 이상적
    - 반면에 독립변수 X와 종속변수 Y의 상관성은 커야 함
    - 위 두 성질을 만족하는 소수의 독립변수 집합
    - 많은 양질의 데이터
- 변수선택 
    - 독립변수 X간에는 상관성이 적고 , X와 종속변수 Y간에 상관성이 큰 독립변수만을 추출
    - 다중선형회귀의 SSE며, 차이는 SSE에 축소하는 항을 추가 
    - 축소항을 어떻게 다룰 것인가.
        - Ridge 회귀 : f(B)에 회귀계수의 제곱합을 대입
            - X'X 의 역행렬을 구할 수 있도록 강제로 작은 diagonal term에 추가한것.
        - Lasso 회귀 : f(B)에 회귀계수의 절대값의 합을 대입
            - 라그랑지안 최적화 기법으로 최적 해를 구함 
        - Elastic-Net 회귀 : Lasso에 적용된 회귀계수의 절대값의 합과 Ridge에 적용된 회귀계수의 제곱의 합을 모두 f(B)에 대입
            - Grouping effect : Lasso는 상관관계가 있는 다수의 변수들 중 하나를 무작위로 선택하여 계수를 축소하는 방면, 상관성이 높은 다수의 변수들을 모두 선택 (또는 제거)
            - 다수의 변수 간에 상관관계가 존재할떄 효과적
    - 람다값을 변화시켜가며 MSE가 최소일 때의 람다를 탐색
    - Ridge와 Lasso의 차이점
        - Ridge는 계수를 소하되 0으로 가까운 수로 축소축, Lasso는 계수를 완전히 0으로 축소
        - Ridge : 입력 변수들이 전반적으로 비슷한 수준으로 출력 변수에 영향을 미치는 경우
        - Lasso : 출력 변수에 미치는 입력 변수의 영향력 편차가 큰 경우
